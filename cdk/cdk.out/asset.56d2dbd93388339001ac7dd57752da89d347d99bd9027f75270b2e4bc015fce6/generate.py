"""
AI ëŒ€í™” ìƒì„± Lambda í•¨ìˆ˜ (LangChain ì ìš© ë²„ì „)
- Runnableê³¼ Memoryë¥¼ ì‚¬ìš©í•˜ì—¬ ëŒ€í™” ê¸°ì–µ ê¸°ëŠ¥ êµ¬í˜„
- í™•ì¥ì„±ê³¼ ìœ ì§€ë³´ìˆ˜ì„±ì´ ë†’ì€ êµ¬ì¡°
- CORS ì˜¤ë¥˜ ìˆ˜ì • ë° ê°„ì†Œí™”
"""
import json
import os
import traceback
import boto3
from datetime import datetime

# --- AWS í´ë¼ì´ì–¸íŠ¸ ë° ê¸°ë³¸ ì„¤ì • ---
bedrock_client = boto3.client("bedrock-runtime", region_name=os.environ.get("REGION", "us-east-1"))
dynamodb_client = boto3.client("dynamodb", region_name=os.environ.get("REGION", "us-east-1"))
PROMPT_META_TABLE = os.environ.get("PROMPT_META_TABLE", "BedrockDiyPrompts")
# ê¸°ë³¸ ëª¨ë¸ ID (í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì§€ì •í•˜ì§€ ì•Šì„ ë•Œ ì‚¬ìš©)
DEFAULT_MODEL_ID = "anthropic.claude-3-sonnet-20240229-v1:0"

# ì§€ì›ë˜ëŠ” ëª¨ë¸ ëª©ë¡
SUPPORTED_MODELS = {
    # Anthropic Claude ëª¨ë¸ë“¤
    "anthropic.claude-opus-4-v1:0": {"name": "Claude Opus 4", "provider": "Anthropic"},
    "anthropic.claude-sonnet-4-v1:0": {"name": "Claude Sonnet 4", "provider": "Anthropic"},
    "anthropic.claude-3-7-sonnet-v1:0": {"name": "Claude 3.7 Sonnet", "provider": "Anthropic"},
    "anthropic.claude-3-5-haiku-20241022-v1:0": {"name": "Claude 3.5 Haiku", "provider": "Anthropic"},
    "anthropic.claude-3-5-sonnet-20241022-v2:0": {"name": "Claude 3.5 Sonnet v2", "provider": "Anthropic"},
    "anthropic.claude-3-5-sonnet-20240620-v1:0": {"name": "Claude 3.5 Sonnet", "provider": "Anthropic"},
    "anthropic.claude-3-opus-20240229-v1:0": {"name": "Claude 3 Opus", "provider": "Anthropic"},
    "anthropic.claude-3-haiku-20240307-v1:0": {"name": "Claude 3 Haiku", "provider": "Anthropic"},
    "anthropic.claude-3-sonnet-20240229-v1:0": {"name": "Claude 3 Sonnet", "provider": "Anthropic"},
    
    # Meta Llama ëª¨ë¸ë“¤
    "meta.llama4-scout-17b-instruct-v4:0": {"name": "Llama 4 Scout 17B", "provider": "Meta"},
    "meta.llama4-maverick-17b-instruct-v4:0": {"name": "Llama 4 Maverick 17B", "provider": "Meta"},
    "meta.llama3-3-70b-instruct-v1:0": {"name": "Llama 3.3 70B", "provider": "Meta"},
    "meta.llama3-2-11b-instruct-v1:0": {"name": "Llama 3.2 11B Vision", "provider": "Meta"},
    "meta.llama3-2-1b-instruct-v1:0": {"name": "Llama 3.2 1B", "provider": "Meta"},
    "meta.llama3-2-3b-instruct-v1:0": {"name": "Llama 3.2 3B", "provider": "Meta"},
    
    # Amazon Nova ëª¨ë¸ë“¤
    "amazon.nova-premier-v1:0": {"name": "Nova Premier", "provider": "Amazon"},
    "amazon.nova-lite-v1:0": {"name": "Nova Lite", "provider": "Amazon"},
    "amazon.nova-micro-v1:0": {"name": "Nova Micro", "provider": "Amazon"},
    "amazon.nova-pro-v1:0": {"name": "Nova Pro", "provider": "Amazon"},
}

def handler(event, context):
    """
    API Gateway ìš”ì²­ì„ ì²˜ë¦¬í•˜ì—¬ Bedrock ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    - GET ìš”ì²­ì€ EventSource (SSE)ë¥¼ ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤ (ê¸´ URL ë¬¸ì œë¡œ í˜„ì¬ëŠ” ë¹„ê¶Œì¥).
    - POST ìš”ì²­ì´ ê¸°ë³¸ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ì…ë‹ˆë‹¤.
    """
    try:
        print(f"ì´ë²¤íŠ¸ ìˆ˜ì‹ : {json.dumps(event)}")
        http_method = event.get("httpMethod", "POST")
        path = event.get("path", "")
        project_id = event.get("pathParameters", {}).get("projectId")

        if not project_id:
            return _create_error_response(400, "í”„ë¡œì íŠ¸ IDê°€ í•„ìš”í•©ë‹ˆë‹¤.")

        # ìš”ì²­ ë³¸ë¬¸(body) íŒŒì‹±
        if http_method == 'GET':
            params = event.get('queryStringParameters') or {}
            user_input = params.get('userInput', '')
            chat_history_str = params.get('chat_history', '[]')
            chat_history = json.loads(chat_history_str)
            model_id = params.get('modelId', DEFAULT_MODEL_ID)
        else: # POST
            body = json.loads(event.get('body', '{}'))
            user_input = body.get('userInput', '')
            chat_history = body.get('chat_history', [])
            prompt_cards = body.get('prompt_cards', [])
            model_id = body.get('modelId', DEFAULT_MODEL_ID)
            
        if not user_input.strip():
            return _create_error_response(400, "ì‚¬ìš©ì ì…ë ¥ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # ëª¨ë¸ ID ê²€ì¦
        if model_id not in SUPPORTED_MODELS:
            print(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ ID: {model_id}")
            model_id = DEFAULT_MODEL_ID
        
        # GET ìš”ì²­ì¼ ë•Œ prompt_cards ì²˜ë¦¬
        if http_method == 'GET':
            prompt_cards = []
        
        print(f"ğŸ¤– ì„ íƒëœ ëª¨ë¸: {model_id} ({SUPPORTED_MODELS.get(model_id, {}).get('name', 'Unknown')})")
        print(f"ğŸ“‹ ìš”ì²­ ë³¸ë¬¸ì—ì„œ ë°›ì€ modelId: {body.get('modelId') if http_method == 'POST' else params.get('modelId')}")
        
        # ìŠ¤íŠ¸ë¦¬ë° ë˜ëŠ” ì¼ë°˜ ìƒì„± ë¶„ê¸°
        if "/stream" in path:
            return _handle_streaming_generation(project_id, user_input, chat_history, prompt_cards, model_id)
        else:
            return _handle_standard_generation(project_id, user_input, chat_history, prompt_cards, model_id)

    except json.JSONDecodeError:
        print("JSON íŒŒì‹± ì˜¤ë¥˜ ë°œìƒ")
        return _create_error_response(400, "ì˜ëª»ëœ JSON í˜•ì‹ì…ë‹ˆë‹¤.")
    except Exception as e:
        print(f"ì˜¤ë¥˜ ë°œìƒ: {traceback.format_exc()}")
        return _create_error_response(500, f"ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜: {e}")

def _handle_streaming_generation(project_id, user_input, chat_history, prompt_cards, model_id):
    """
    Bedrockì—ì„œ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ë°›ì•„ ì‹¤ì‹œê°„ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì²­í¬ë³„ë¡œ ì¦‰ì‹œ SSE í˜•ì‹ìœ¼ë¡œ êµ¬ì„±í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        print(f"ìŠ¤íŠ¸ë¦¬ë° ìƒì„± ì‹œì‘: í”„ë¡œì íŠ¸ ID={project_id}, ëª¨ë¸={model_id}")
        final_prompt = _build_final_prompt(project_id, user_input, chat_history, prompt_cards)
        
        # ëª¨ë¸ì— ë”°ë¥¸ ìš”ì²­ ë³¸ë¬¸ êµ¬ì„±
        if model_id.startswith("anthropic."):
            request_body = {
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 4096,
                "messages": [{"role": "user", "content": final_prompt}],
                "temperature": 0.1,
                "top_p": 0.9,
            }
        else:
            # Meta Llamaë‚˜ Amazon Nova ëª¨ë¸ë“¤ì„ ìœ„í•œ ìš”ì²­ í˜•ì‹
            request_body = {
                "prompt": final_prompt,
                "max_gen_len": 4096,
                "temperature": 0.1,
                "top_p": 0.9,
            }

        response_stream = bedrock_client.invoke_model_with_response_stream(
            modelId=model_id,
            body=json.dumps(request_body)
        )
        
        # ğŸ’¡ ìµœì í™”ëœ ìŠ¤íŠ¸ë¦¬ë° êµ¬í˜„ - ë²„í¼ë§ ìµœì†Œí™”
        sse_chunks = []
        full_response = ""
        
        # ì‹œì‘ ì´ë²¤íŠ¸
        start_data = {
            "response": "",
            "sessionId": project_id,
            "type": "start"
        }
        sse_chunks.append(f"data: {json.dumps(start_data)}\n\n")
        
        # ì‹¤ì‹œê°„ ì²­í¬ ì²˜ë¦¬ - ìµœì†Œ ì§€ì—°
        for event in response_stream.get("body"):
            chunk = json.loads(event["chunk"]["bytes"].decode())
            
            # ëª¨ë¸ë³„ ì‘ë‹µ í˜•ì‹ ì²˜ë¦¬
            text = None
            if model_id.startswith("anthropic."):
                # Anthropic ëª¨ë¸ ì‘ë‹µ í˜•ì‹
                if chunk['type'] == 'content_block_delta':
                    text = chunk['delta']['text']
            else:
                # Meta Llamaë‚˜ Amazon Nova ëª¨ë¸ ì‘ë‹µ í˜•ì‹
                if 'generation' in chunk:
                    text = chunk['generation']
                elif 'text' in chunk:
                    text = chunk['text']
            
            if text:
                full_response += text
                
                # ì¦‰ì‹œ ì²­í¬ ì „ì†¡ (ë²„í¼ë§ ì—†ìŒ)
                sse_data = {
                    "response": text,
                    "sessionId": project_id,
                    "type": "chunk"
                }
                sse_chunks.append(f"data: {json.dumps(sse_data)}\n\n")
        
        # ì™„ë£Œ ì´ë²¤íŠ¸ ì „ì†¡
        completion_data = {
            "response": "",
            "sessionId": project_id,
            "type": "complete",
            "fullResponse": full_response
        }
        sse_chunks.append(f"data: {json.dumps(completion_data)}\n\n")
        
        print(f"ìŠ¤íŠ¸ë¦¬ë° ìƒì„± ì™„ë£Œ: ì´ {len(sse_chunks)} ì²­í¬ ìƒì„±ë¨, ì‘ë‹µ ê¸¸ì´={len(full_response)}")
        return {
            "statusCode": 200,
            "headers": _get_sse_headers(),
            "body": "".join(sse_chunks),
            "isBase64Encoded": False
        }

    except Exception as e:
        print(f"ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {traceback.format_exc()}")
        error_data = {
            "error": str(e),
            "sessionId": project_id,
            "type": "error"
        }
        return {
            "statusCode": 500,
            "headers": _get_sse_headers(),
            "body": f"data: {json.dumps(error_data)}\n\n",
            "isBase64Encoded": False
        }

def _handle_standard_generation(project_id, user_input, chat_history, prompt_cards, model_id):
    """ì¼ë°˜(non-streaming) Bedrock ì‘ë‹µì„ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    try:
        print(f"ì¼ë°˜ ìƒì„± ì‹œì‘: í”„ë¡œì íŠ¸ ID={project_id}, ëª¨ë¸={model_id}")
        final_prompt = _build_final_prompt(project_id, user_input, chat_history, prompt_cards)
        
        # ëª¨ë¸ì— ë”°ë¥¸ ìš”ì²­ ë³¸ë¬¸ êµ¬ì„±
        if model_id.startswith("anthropic."):
            request_body = {
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 4096,
                "messages": [{"role": "user", "content": final_prompt}],
                "temperature": 0.1,
                "top_p": 0.9
            }
        else:
            # Meta Llamaë‚˜ Amazon Nova ëª¨ë¸ë“¤ì„ ìœ„í•œ ìš”ì²­ í˜•ì‹
            request_body = {
                "prompt": final_prompt,
                "max_gen_len": 4096,
                "temperature": 0.1,
                "top_p": 0.9,
            }

        response = bedrock_client.invoke_model(
            modelId=model_id,
            body=json.dumps(request_body)
        )
        response_body = json.loads(response['body'].read())
        
        # ëª¨ë¸ë³„ ì‘ë‹µ í˜•ì‹ ì²˜ë¦¬
        if model_id.startswith("anthropic."):
            # Anthropic ëª¨ë¸ ì‘ë‹µ í˜•ì‹
            result_text = response_body['content'][0]['text']
        else:
            # Meta Llamaë‚˜ Amazon Nova ëª¨ë¸ ì‘ë‹µ í˜•ì‹
            if 'generation' in response_body:
                result_text = response_body['generation']
            elif 'outputs' in response_body:
                result_text = response_body['outputs'][0]['text']
            else:
                result_text = response_body.get('text', str(response_body))
        
        print(f"ì¼ë°˜ ìƒì„± ì™„ë£Œ: ì‘ë‹µ ê¸¸ì´={len(result_text)}")
        return {
            "statusCode": 200,
            "headers": {"Content-Type": "application/json", "Access-Control-Allow-Origin": "*"},
            "body": json.dumps({"result": result_text}),
            "isBase64Encoded": False
        }
    except Exception as e:
        print(f"ì¼ë°˜ ìƒì„± ì˜¤ë¥˜: {traceback.format_exc()}")
        return _create_error_response(500, f"Bedrock í˜¸ì¶œ ì˜¤ë¥˜: {e}")

def _build_final_prompt(project_id, user_input, chat_history, prompt_cards):
    """í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì „ì†¡ëœ í”„ë¡¬í”„íŠ¸ ì¹´ë“œì™€ ì±„íŒ… íˆìŠ¤í† ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì¢… í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤."""
    try:
        print(f"í”„ë¡¬í”„íŠ¸ êµ¬ì„± ì‹œì‘: í”„ë¡œì íŠ¸ ID={project_id}")
        print(f"ì „ë‹¬ë°›ì€ í”„ë¡¬í”„íŠ¸ ì¹´ë“œ ìˆ˜: {len(prompt_cards)}")
        print(f"ì „ë‹¬ë°›ì€ ì±„íŒ… íˆìŠ¤í† ë¦¬ ìˆ˜: {len(chat_history)}")
        
        # í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì „ì†¡ëœ í”„ë¡¬í”„íŠ¸ ì¹´ë“œ ì‚¬ìš© (ì´ë¯¸ í™œì„±í™”ëœ ê²ƒë§Œ í•„í„°ë§ë˜ì–´ ì „ì†¡ë¨)
        system_prompt_parts = []
        for card in prompt_cards:
            prompt_text = card.get('prompt_text', '').strip()
            if prompt_text:
                title = card.get('title', 'Untitled')
                print(f"í”„ë¡¬í”„íŠ¸ ì¹´ë“œ ì ìš©: '{title}' ({len(prompt_text)}ì)")
                system_prompt_parts.append(prompt_text)
        
        system_prompt = "\n\n".join(system_prompt_parts)
        print(f"ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(system_prompt)}ì")
        
        # ì±„íŒ… íˆìŠ¤í† ë¦¬ êµ¬ì„±
        history_parts = []
        for msg in chat_history:
            role = msg.get('role', '')
            content = msg.get('content', '')
            if role and content:
                if role == 'user':
                    history_parts.append(f"Human: {content}")
                elif role == 'assistant':
                    history_parts.append(f"Assistant: {content}")
        
        history_str = "\n\n".join(history_parts)
        print(f"ì±„íŒ… íˆìŠ¤í† ë¦¬ ê¸¸ì´: {len(history_str)}ì")
        
        # ìµœì¢… í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt_parts = []
        
        # 1. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì—­í• , ì§€ì¹¨ ë“±)
        if system_prompt:
            prompt_parts.append(system_prompt)
        
        # 2. ëŒ€í™” íˆìŠ¤í† ë¦¬
        if history_str:
            prompt_parts.append(history_str)
        
        # 3. í˜„ì¬ ì‚¬ìš©ì ì…ë ¥
        prompt_parts.append(f"Human: {user_input}")
        prompt_parts.append("Assistant:")
        
        final_prompt = "\n\n".join(prompt_parts)
        print(f"ìµœì¢… í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(final_prompt)}ì")
        
        return final_prompt
        
    except Exception as e:
        print(f"í”„ë¡¬í”„íŠ¸ êµ¬ì„± ì˜¤ë¥˜: {traceback.format_exc()}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ë°˜í™˜ (íˆìŠ¤í† ë¦¬ í¬í•¨)
        try:
            history_str = "\n\n".join([f"{msg['role']}: {msg['content']}" for msg in chat_history])
            if history_str:
                return f"{history_str}\n\nHuman: {user_input}\n\nAssistant:"
            else:
                return f"Human: {user_input}\n\nAssistant:"
        except:
            return f"Human: {user_input}\n\nAssistant:"

def _get_sse_headers():
    """Server-Sent Events ì‘ë‹µì„ ìœ„í•œ í—¤ë”ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    return {
        'Content-Type': 'text/event-stream',
        'Cache-Control': 'no-cache',
        'Connection': 'keep-alive',
        'Access-Control-Allow-Origin': '*',
        'X-Accel-Buffering': 'no'  # NGINX ë²„í¼ë§ ë¹„í™œì„±í™”
    }

def _create_error_response(status_code, message):
    """ì¼ë°˜ì ì¸ JSON ì˜¤ë¥˜ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
    return {
        "statusCode": status_code,
        "headers": {"Content-Type": "application/json", "Access-Control-Allow-Origin": "*"},
        "body": json.dumps({"error": message, "timestamp": datetime.utcnow().isoformat()}),
        "isBase64Encoded": False
    } 