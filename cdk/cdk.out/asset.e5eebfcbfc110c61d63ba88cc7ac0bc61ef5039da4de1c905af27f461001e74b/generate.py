"""
AI ëŒ€í™” ìƒì„± Lambda í•¨ìˆ˜ (LangChain ì ìš© ë²„ì „)
- Runnableê³¼ Memoryë¥¼ ì‚¬ìš©í•˜ì—¬ ëŒ€í™” ê¸°ì–µ ê¸°ëŠ¥ êµ¬í˜„
- í™•ì¥ì„±ê³¼ ìœ ì§€ë³´ìˆ˜ì„±ì´ ë†’ì€ êµ¬ì¡°
- CORS ì˜¤ë¥˜ ìˆ˜ì • ë° ê°„ì†Œí™”
"""
import json
import os
import traceback
import boto3
from datetime import datetime

# --- AWS í´ë¼ì´ì–¸íŠ¸ ë° ê¸°ë³¸ ì„¤ì • ---
bedrock_client = boto3.client("bedrock-runtime", region_name=os.environ.get("REGION", "us-east-1"))
dynamodb_client = boto3.client("dynamodb", region_name=os.environ.get("REGION", "us-east-1"))
PROMPT_META_TABLE = os.environ.get("PROMPT_META_TABLE", "BedrockDiyPrompts")
MODEL_ID = "anthropic.claude-3-sonnet-20240229-v1:0"

def handler(event, context):
    """
    API Gateway ìš”ì²­ì„ ì²˜ë¦¬í•˜ì—¬ Bedrock ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    - GET ìš”ì²­ì€ EventSource (SSE)ë¥¼ ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤ (ê¸´ URL ë¬¸ì œë¡œ í˜„ì¬ëŠ” ë¹„ê¶Œì¥).
    - POST ìš”ì²­ì´ ê¸°ë³¸ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ì…ë‹ˆë‹¤.
    """
    try:
        print(f"ì´ë²¤íŠ¸ ìˆ˜ì‹ : {json.dumps(event)}")
        http_method = event.get("httpMethod", "POST")
        path = event.get("path", "")
        project_id = event.get("pathParameters", {}).get("projectId")

        if not project_id:
            return _create_error_response(400, "í”„ë¡œì íŠ¸ IDê°€ í•„ìš”í•©ë‹ˆë‹¤.")

        # ìš”ì²­ ë³¸ë¬¸(body) íŒŒì‹±
        if http_method == 'GET':
            params = event.get('queryStringParameters') or {}
            user_input = params.get('userInput', '')
            chat_history_str = params.get('chat_history', '[]')
            chat_history = json.loads(chat_history_str)
        else: # POST
            body = json.loads(event.get('body', '{}'))
            user_input = body.get('userInput', '')
            chat_history = body.get('chat_history', [])
            prompt_cards = body.get('prompt_cards', [])
            
        if not user_input.strip():
            return _create_error_response(400, "ì‚¬ìš©ì ì…ë ¥ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # GET ìš”ì²­ì¼ ë•Œ prompt_cards ì²˜ë¦¬
        if http_method == 'GET':
            prompt_cards = []
        
        # ìŠ¤íŠ¸ë¦¬ë° ë˜ëŠ” ì¼ë°˜ ìƒì„± ë¶„ê¸°
        if "/stream" in path:
            return _handle_streaming_generation(project_id, user_input, chat_history, prompt_cards)
        else:
            return _handle_standard_generation(project_id, user_input, chat_history, prompt_cards)

    except json.JSONDecodeError:
        print("JSON íŒŒì‹± ì˜¤ë¥˜ ë°œìƒ")
        return _create_error_response(400, "ì˜ëª»ëœ JSON í˜•ì‹ì…ë‹ˆë‹¤.")
    except Exception as e:
        print(f"ì˜¤ë¥˜ ë°œìƒ: {traceback.format_exc()}")
        return _create_error_response(500, f"ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜: {e}")

def _handle_streaming_generation(project_id, user_input, chat_history, prompt_cards):
    """
    Bedrockì—ì„œ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ë°›ì•„ ì‹¤ì‹œê°„ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì²­í¬ë³„ë¡œ ì¦‰ì‹œ SSE í˜•ì‹ìœ¼ë¡œ êµ¬ì„±í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        print(f"ìŠ¤íŠ¸ë¦¬ë° ìƒì„± ì‹œì‘: í”„ë¡œì íŠ¸ ID={project_id}")
        final_prompt = _build_final_prompt(project_id, user_input, chat_history, prompt_cards)
        
        request_body = {
            "anthropic_version": "bedrock-2023-05-31",
            "max_tokens": 4096,
            "messages": [{"role": "user", "content": final_prompt}],
            "temperature": 0.3,
            "top_p": 0.9,
        }

        response_stream = bedrock_client.invoke_model_with_response_stream(
            modelId=MODEL_ID,
            body=json.dumps(request_body)
        )
        
        # ğŸ’¡ ìµœì í™”ëœ ìŠ¤íŠ¸ë¦¬ë° êµ¬í˜„ - ë²„í¼ë§ ìµœì†Œí™”
        sse_chunks = []
        full_response = ""
        
        # ì‹œì‘ ì´ë²¤íŠ¸
        start_data = {
            "response": "",
            "sessionId": project_id,
            "type": "start"
        }
        sse_chunks.append(f"data: {json.dumps(start_data)}\n\n")
        
        # ì‹¤ì‹œê°„ ì²­í¬ ì²˜ë¦¬ - ìµœì†Œ ì§€ì—°
        for event in response_stream.get("body"):
            chunk = json.loads(event["chunk"]["bytes"].decode())
            if chunk['type'] == 'content_block_delta':
                text = chunk['delta']['text']
                full_response += text
                
                # ì¦‰ì‹œ ì²­í¬ ì „ì†¡ (ë²„í¼ë§ ì—†ìŒ)
                sse_data = {
                    "response": text,
                    "sessionId": project_id,
                    "type": "chunk"
                }
                sse_chunks.append(f"data: {json.dumps(sse_data)}\n\n")
        
        # ì™„ë£Œ ì´ë²¤íŠ¸ ì „ì†¡
        completion_data = {
            "response": "",
            "sessionId": project_id,
            "type": "complete",
            "fullResponse": full_response
        }
        sse_chunks.append(f"data: {json.dumps(completion_data)}\n\n")
        
        print(f"ìŠ¤íŠ¸ë¦¬ë° ìƒì„± ì™„ë£Œ: ì´ {len(sse_chunks)} ì²­í¬ ìƒì„±ë¨, ì‘ë‹µ ê¸¸ì´={len(full_response)}")
        return {
            "statusCode": 200,
            "headers": _get_sse_headers(),
            "body": "".join(sse_chunks),
            "isBase64Encoded": False
        }

    except Exception as e:
        print(f"ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {traceback.format_exc()}")
        error_data = {
            "error": str(e),
            "sessionId": project_id,
            "type": "error"
        }
        return {
            "statusCode": 500,
            "headers": _get_sse_headers(),
            "body": f"data: {json.dumps(error_data)}\n\n",
            "isBase64Encoded": False
        }

def _handle_standard_generation(project_id, user_input, chat_history, prompt_cards):
    """ì¼ë°˜(non-streaming) Bedrock ì‘ë‹µì„ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    try:
        print(f"ì¼ë°˜ ìƒì„± ì‹œì‘: í”„ë¡œì íŠ¸ ID={project_id}")
        final_prompt = _build_final_prompt(project_id, user_input, chat_history, prompt_cards)
        
        request_body = {
            "anthropic_version": "bedrock-2023-05-31",
            "max_tokens": 4096,
            "messages": [{"role": "user", "content": final_prompt}],
            "temperature": 0.3,
            "top_p": 0.9
        }

        response = bedrock_client.invoke_model(
            modelId=MODEL_ID,
            body=json.dumps(request_body)
        )
        response_body = json.loads(response['body'].read())
        result_text = response_body['content'][0]['text']
        
        print(f"ì¼ë°˜ ìƒì„± ì™„ë£Œ: ì‘ë‹µ ê¸¸ì´={len(result_text)}")
        return {
            "statusCode": 200,
            "headers": {"Content-Type": "application/json", "Access-Control-Allow-Origin": "*"},
            "body": json.dumps({"result": result_text}),
            "isBase64Encoded": False
        }
    except Exception as e:
        print(f"ì¼ë°˜ ìƒì„± ì˜¤ë¥˜: {traceback.format_exc()}")
        return _create_error_response(500, f"Bedrock í˜¸ì¶œ ì˜¤ë¥˜: {e}")

def _build_final_prompt(project_id, user_input, chat_history, prompt_cards):
    """í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì „ì†¡ëœ í”„ë¡¬í”„íŠ¸ ì¹´ë“œì™€ ì±„íŒ… íˆìŠ¤í† ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì¢… í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤."""
    try:
        print(f"í”„ë¡¬í”„íŠ¸ êµ¬ì„± ì‹œì‘: í”„ë¡œì íŠ¸ ID={project_id}")
        print(f"ì „ë‹¬ë°›ì€ í”„ë¡¬í”„íŠ¸ ì¹´ë“œ ìˆ˜: {len(prompt_cards)}")
        print(f"ì „ë‹¬ë°›ì€ ì±„íŒ… íˆìŠ¤í† ë¦¬ ìˆ˜: {len(chat_history)}")
        
        # í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì „ì†¡ëœ í”„ë¡¬í”„íŠ¸ ì¹´ë“œ ì‚¬ìš© (ì´ë¯¸ í™œì„±í™”ëœ ê²ƒë§Œ í•„í„°ë§ë˜ì–´ ì „ì†¡ë¨)
        system_prompt_parts = []
        for card in prompt_cards:
            prompt_text = card.get('prompt_text', '').strip()
            if prompt_text:
                title = card.get('title', 'Untitled')
                print(f"í”„ë¡¬í”„íŠ¸ ì¹´ë“œ ì ìš©: '{title}' ({len(prompt_text)}ì)")
                system_prompt_parts.append(prompt_text)
        
        system_prompt = "\n\n".join(system_prompt_parts)
        print(f"ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(system_prompt)}ì")
        
        # ì±„íŒ… íˆìŠ¤í† ë¦¬ êµ¬ì„±
        history_parts = []
        for msg in chat_history:
            role = msg.get('role', '')
            content = msg.get('content', '')
            if role and content:
                if role == 'user':
                    history_parts.append(f"Human: {content}")
                elif role == 'assistant':
                    history_parts.append(f"Assistant: {content}")
        
        history_str = "\n\n".join(history_parts)
        print(f"ì±„íŒ… íˆìŠ¤í† ë¦¬ ê¸¸ì´: {len(history_str)}ì")
        
        # ìµœì¢… í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt_parts = []
        
        # 1. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì—­í• , ì§€ì¹¨ ë“±)
        if system_prompt:
            prompt_parts.append(system_prompt)
        
        # 2. ëŒ€í™” íˆìŠ¤í† ë¦¬
        if history_str:
            prompt_parts.append(history_str)
        
        # 3. í˜„ì¬ ì‚¬ìš©ì ì…ë ¥
        prompt_parts.append(f"Human: {user_input}")
        prompt_parts.append("Assistant:")
        
        final_prompt = "\n\n".join(prompt_parts)
        print(f"ìµœì¢… í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(final_prompt)}ì")
        
        return final_prompt
        
    except Exception as e:
        print(f"í”„ë¡¬í”„íŠ¸ êµ¬ì„± ì˜¤ë¥˜: {traceback.format_exc()}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ë°˜í™˜ (íˆìŠ¤í† ë¦¬ í¬í•¨)
        try:
            history_str = "\n\n".join([f"{msg['role']}: {msg['content']}" for msg in chat_history])
            if history_str:
                return f"{history_str}\n\nHuman: {user_input}\n\nAssistant:"
            else:
                return f"Human: {user_input}\n\nAssistant:"
        except:
            return f"Human: {user_input}\n\nAssistant:"

def _get_sse_headers():
    """Server-Sent Events ì‘ë‹µì„ ìœ„í•œ í—¤ë”ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    return {
        'Content-Type': 'text/event-stream',
        'Cache-Control': 'no-cache',
        'Connection': 'keep-alive',
        'Access-Control-Allow-Origin': '*',
        'X-Accel-Buffering': 'no'  # NGINX ë²„í¼ë§ ë¹„í™œì„±í™”
    }

def _create_error_response(status_code, message):
    """ì¼ë°˜ì ì¸ JSON ì˜¤ë¥˜ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
    return {
        "statusCode": status_code,
        "headers": {"Content-Type": "application/json", "Access-Control-Allow-Origin": "*"},
        "body": json.dumps({"error": message, "timestamp": datetime.utcnow().isoformat()}),
        "isBase64Encoded": False
    } 