"""
WebSocket ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° Lambda í•¨ìˆ˜
"""
import json
import os
import boto3
import traceback
from datetime import datetime, timezone

# AWS í´ë¼ì´ì–¸íŠ¸
bedrock_client = boto3.client("bedrock-runtime")
dynamodb_client = boto3.client("dynamodb")
dynamodb_resource = boto3.resource("dynamodb")
apigateway_client = boto3.client("apigatewaymanagementapi")

# í™˜ê²½ ë³€ìˆ˜
CONNECTIONS_TABLE = os.environ.get('CONNECTIONS_TABLE')
PROMPT_META_TABLE = os.environ.get('PROMPT_META_TABLE')
PROMPT_BUCKET = os.environ.get('PROMPT_BUCKET')
CONVERSATIONS_TABLE = os.environ.get('CONVERSATIONS_TABLE', 'Conversations')
MESSAGES_TABLE = os.environ.get('MESSAGES_TABLE', 'Messages')
MODEL_ID = "apac.anthropic.claude-sonnet-4-20250514-v1:0"

# DynamoDB tables
conversations_table = dynamodb_resource.Table(CONVERSATIONS_TABLE)
messages_table = dynamodb_resource.Table(MESSAGES_TABLE)

# ì²­í¬ ë°ì´í„° ì„ì‹œ ì €ì¥ì†Œ (Lambda ë©”ëª¨ë¦¬ì— ì €ì¥)
chunk_storage = {}

def handler(event, context):
    """
    WebSocket ìŠ¤íŠ¸ë¦¬ë° ë©”ì‹œì§€ ì²˜ë¦¬
    """
    try:
        connection_id = event['requestContext']['connectionId']
        domain_name = event['requestContext']['domainName']
        stage = event['requestContext']['stage']
        
        # API Gateway Management API í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
        endpoint_url = f"https://{domain_name}/{stage}"
        global apigateway_client
        apigateway_client = boto3.client(
            'apigatewaymanagementapi',
            endpoint_url=endpoint_url
        )
        
        # ìš”ì²­ ë³¸ë¬¸ íŒŒì‹±
        body = json.loads(event.get('body', '{}'))
        action = body.get('action')
        
        if action == 'stream':
            return handle_stream_request(connection_id, body)
        elif action == 'stream_chunk':
            return handle_chunk_request(connection_id, body)
        else:
            return send_error(connection_id, "ì§€ì›í•˜ì§€ ì•ŠëŠ” ì•¡ì…˜ì…ë‹ˆë‹¤")
            
    except Exception as e:
        print(f"WebSocket ì²˜ë¦¬ ì˜¤ë¥˜: {traceback.format_exc()}")
        return {
            'statusCode': 500,
            'body': json.dumps({'error': str(e)})
        }

def handle_chunk_request(connection_id, data):
    """
    ì²­í¬ë¡œ ë¶„í• ëœ ë©”ì‹œì§€ ì²˜ë¦¬
    """
    try:
        chunk_id = data.get('chunkId')
        chunk_index = data.get('chunkIndex')
        total_chunks = data.get('totalChunks')
        chunk_data = data.get('chunkData')
        is_complete = data.get('isComplete', False)
        
        print(f"ğŸ” [DEBUG] ì²­í¬ ìˆ˜ì‹ : ID={chunk_id}, Index={chunk_index}/{total_chunks}")
        
        # ì²­í¬ ì €ì¥
        if chunk_id not in chunk_storage:
            chunk_storage[chunk_id] = {
                'chunks': {},
                'metadata': {},
                'connection_id': connection_id
            }
        
        chunk_storage[chunk_id]['chunks'][chunk_index] = chunk_data
        
        # ëª¨ë“  ì²­í¬ê°€ ë„ì°©í–ˆëŠ”ì§€ í™•ì¸
        if is_complete and len(chunk_storage[chunk_id]['chunks']) == total_chunks:
            print(f"ğŸ” [DEBUG] ëª¨ë“  ì²­í¬ ìˆ˜ì‹  ì™„ë£Œ, ì¬ì¡°í•© ì‹œì‘")
            
            # ì²­í¬ ì¬ì¡°í•©
            full_text = ''
            for i in range(total_chunks):
                full_text += chunk_storage[chunk_id]['chunks'].get(i, '')
            
            # ì›ë³¸ ë©”íƒ€ë°ì´í„° ë³µì› (ì²« ë²ˆì§¸ ë©”ì‹œì§€ì—ì„œ ì €ì¥ëœ ê²ƒ)
            metadata = chunk_storage[chunk_id].get('metadata', {})
            
            # ì¬ì¡°í•©ëœ ë°ì´í„°ë¡œ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬
            reconstructed_data = {
                'userInput': full_text,
                'chat_history': metadata.get('chat_history', []),
                'prompt_cards': metadata.get('prompt_cards', []),
                'conversationId': metadata.get('conversationId'),
                'userSub': metadata.get('userSub'),
                'enableStepwise': metadata.get('enableStepwise', False)
            }
            
            # ì²­í¬ ì €ì¥ì†Œ ì •ë¦¬
            del chunk_storage[chunk_id]
            
            # ì¼ë°˜ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ë¡œ ì „ë‹¬
            return handle_stream_request(connection_id, reconstructed_data)
        
        return {
            'statusCode': 200,
            'body': json.dumps({'message': f'ì²­í¬ {chunk_index + 1}/{total_chunks} ìˆ˜ì‹  ì™„ë£Œ'})
        }
        
    except Exception as e:
        print(f"ì²­í¬ ì²˜ë¦¬ ì˜¤ë¥˜: {traceback.format_exc()}")
        return send_error(connection_id, f"ì²­í¬ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")

def handle_stream_request(connection_id, data):
    """
    ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ìš”ì²­ ì²˜ë¦¬ - ë‹¨ê³„ë³„ ì‹¤í–‰ ë° ì‚¬ê³ ê³¼ì • í¬í•¨
    """
    try:
        # ì²­í¬ ë¶„í• ëœ ë©”ì‹œì§€ì¸ì§€ í™•ì¸
        if data.get('chunked', False):
            chunk_id = data.get('chunkId')
            chunk_index = data.get('chunkIndex')
            total_chunks = data.get('totalChunks')
            
            print(f"ğŸ” [DEBUG] ì²­í¬ ë©”ì‹œì§€ ê°ì§€: {chunk_index + 1}/{total_chunks}")
            
            # ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ê°œì„ : ì¦‰ì‹œ ì²˜ë¦¬ ì‹œì‘
            if chunk_index == 0:
                if chunk_id not in chunk_storage:
                    chunk_storage[chunk_id] = {
                        'chunks': {},
                        'metadata': {},
                        'connection_id': connection_id,
                        'streaming_started': False,
                        'processed_chunks': 0
                    }
                
                # ë©”íƒ€ë°ì´í„° ì €ì¥
                chunk_storage[chunk_id]['metadata'] = {
                    'chat_history': data.get('chat_history', []),
                    'prompt_cards': data.get('prompt_cards', []),
                    'conversationId': data.get('conversationId'),
                    'userSub': data.get('userSub'),
                    'enableStepwise': data.get('enableStepwise', False)
                }
                
                # ì²« ë²ˆì§¸ ì²­í¬ ì €ì¥
                chunk_storage[chunk_id]['chunks'][0] = data.get('userInput')
                
                # ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘
                if total_chunks > 3:  # 3ê°œ ì´ìƒì˜ ì²­í¬ì¸ ê²½ìš° ë³‘ë ¬ ì²˜ë¦¬
                    _start_parallel_chunk_processing(connection_id, chunk_id, total_chunks)
                
                # ì¶”ê°€ ì²­í¬ ëŒ€ê¸° ë©”ì‹œì§€
                send_message(connection_id, {
                    "type": "progress",
                    "step": f"ğŸ“¦ ëŒ€ìš©ëŸ‰ í…ìŠ¤íŠ¸ ìˆ˜ì‹  ì¤‘... (1/{total_chunks})",
                    "progress": int((1 / total_chunks) * 100)
                })
                
                return {
                    'statusCode': 200,
                    'body': json.dumps({'message': 'ì²« ë²ˆì§¸ ì²­í¬ ìˆ˜ì‹  ì™„ë£Œ'})
                }
        
        # ì¼ë°˜ ë©”ì‹œì§€ ì²˜ë¦¬
        user_input = data.get('userInput')
        chat_history = data.get('chat_history', [])
        prompt_cards = data.get('prompt_cards', [])
        conversation_id = data.get('conversationId')
        user_sub = data.get('userSub')
        enable_stepwise = data.get('enableStepwise', False)  # ë‹¨ê³„ë³„ ì‹¤í–‰ ì˜µì…˜
        
        print(f"ğŸ” [DEBUG] WebSocket ìŠ¤íŠ¸ë¦¼ ìš”ì²­ ë°›ìŒ:")
        print(f"  - user_input: {user_input[:50]}..." if user_input else "  - user_input: None")
        print(f"  - user_input length: {len(user_input) if user_input else 0}")
        print(f"  - enable_stepwise: {enable_stepwise}")
        print(f"  - prompt_cards count: {len(prompt_cards)}")
        
        if not user_input:
            return send_error(connection_id, "ì‚¬ìš©ì ì…ë ¥ì´ í•„ìš”í•©ë‹ˆë‹¤")
        
        # ë‹¨ê³„ë³„ ì‹¤í–‰ ëª¨ë“œ
        if enable_stepwise and prompt_cards and len(prompt_cards) > 0:
            return handle_stepwise_execution(connection_id, user_input, prompt_cards, chat_history, conversation_id, user_sub)
        
        # 1ë‹¨ê³„: í”„ë¡¬í”„íŠ¸ êµ¬ì„± ì‹œì‘
        send_message(connection_id, {
            "type": "progress",
            "step": "ğŸ”§ í”„ë¡¬í”„íŠ¸ ì¹´ë“œë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
            "progress": 10
        })
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        final_prompt = build_final_prompt(user_input, chat_history, prompt_cards)
        
        # í”„ë¡¬í”„íŠ¸ í¬ê¸° í™•ì¸
        print(f"ğŸ” [DEBUG] ìµœì¢… í”„ë¡¬í”„íŠ¸ í¬ê¸°: {len(final_prompt)}ì ({len(final_prompt) / 1024:.2f}KB)")
        
        # í”„ë¡¬í”„íŠ¸ê°€ ë„ˆë¬´ í° ê²½ìš° ì²˜ë¦¬ - ì œê±° (build_final_promptì—ì„œ ì²˜ë¦¬í•¨)
        # MAX_PROMPT_SIZE = 200000  # 200KB ì œí•œ (ì•ˆì „í•œ ë²”ìœ„)
        # if len(final_prompt) > MAX_PROMPT_SIZE:
        #     print(f"âš ï¸ [WARNING] í”„ë¡¬í”„íŠ¸ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤. ì˜ë¼ì„œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        #     # ì±„íŒ… íˆìŠ¤í† ë¦¬ë¥¼ ì¤„ì´ê±°ë‚˜ user_inputë§Œ ì‚¬ìš©
        #     final_prompt = build_final_prompt(user_input[:MAX_PROMPT_SIZE], [], prompt_cards)
        
        # 2ë‹¨ê³„: AI ëª¨ë¸ ì¤€ë¹„
        send_message(connection_id, {
            "type": "progress", 
            "step": "ğŸ¤– AI ëª¨ë¸ì„ ì¤€ë¹„í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
            "progress": 25
        })
        
        # Bedrock ìŠ¤íŠ¸ë¦¬ë° ìš”ì²­
        request_body = {
            "anthropic_version": "bedrock-2023-05-31",
            "max_tokens": 4096,
            "messages": [{"role": "user", "content": final_prompt}],
            "temperature": 0.3,
            "top_p": 0.9,
        }
        
        # 3ë‹¨ê³„: ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘
        send_message(connection_id, {
            "type": "progress",
            "step": "âœï¸ AIê°€ ì‘ë‹µì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
            "progress": 40
        })
        
        try:
            # Bedrock ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
            response_stream = bedrock_client.invoke_model_with_response_stream(
                modelId=MODEL_ID,
                body=json.dumps(request_body)
            )
        except Exception as bedrock_error:
            print(f"âŒ [ERROR] Bedrock API í˜¸ì¶œ ì‹¤íŒ¨: {str(bedrock_error)}")
            print(f"Request body size: {len(json.dumps(request_body))} bytes")
            
            # ì—ëŸ¬ íƒ€ì…ì— ë”°ë¥¸ ì²˜ë¦¬
            error_message = str(bedrock_error)
            if "ValidationException" in error_message:
                if "maximum" in error_message.lower() or "token" in error_message.lower():
                    send_error(connection_id, "ì…ë ¥ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤. í…ìŠ¤íŠ¸ë¥¼ ì¤„ì—¬ì„œ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
                else:
                    send_error(connection_id, "ì…ë ¥ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            elif "ThrottlingException" in error_message:
                send_error(connection_id, "ìš”ì²­ì´ ë„ˆë¬´ ë§ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
            else:
                send_error(connection_id, f"AI ëª¨ë¸ í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_message}")
            
            return {
                'statusCode': 400,
                'body': json.dumps({'error': str(bedrock_error)})
            }
        
        full_response = ""
        
        # ì‹¤ì‹œê°„ ì²­í¬ ì „ì†¡
        for event in response_stream.get("body"):
            chunk = json.loads(event["chunk"]["bytes"].decode())
            
            if chunk['type'] == 'content_block_delta':
                text = chunk['delta']['text']
                full_response += text
                
                # ì¦‰ì‹œ í´ë¼ì´ì–¸íŠ¸ë¡œ ì „ì†¡
                send_message(connection_id, {
                    "type": "stream_chunk",
                    "content": text
                })
        
        # 4ë‹¨ê³„: ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ
        send_message(connection_id, {
            "type": "progress",
            "step": "âœ… ì‘ë‹µ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!",
            "progress": 100
        })
        
        # ìµœì¢… ì™„ë£Œ ì•Œë¦¼
        send_message(connection_id, {
            "type": "stream_complete", 
            "fullContent": full_response
        })
        
        # ë©”ì‹œì§€ ì €ì¥ (conversation_idì™€ user_subê°€ ìˆëŠ” ê²½ìš°)
        if conversation_id and user_sub:
            print(f"ğŸ” [DEBUG] ë©”ì‹œì§€ ì €ì¥ ì‹œì‘:")
            print(f"  - conversation_id: {conversation_id}")
            print(f"  - user_sub: {user_sub}")
            print(f"  - user_input length: {len(user_input)}")
            print(f"  - assistant_response length: {len(full_response)}")
            save_conversation_messages(conversation_id, user_sub, user_input, full_response)
        else:
            print(f"ğŸ” [DEBUG] ë©”ì‹œì§€ ì €ì¥ ê±´ë„ˆëœ€:")
            print(f"  - conversation_id: {conversation_id} (is None: {conversation_id is None})")
            print(f"  - user_sub: {user_sub} (is None: {user_sub is None})")
            print(f"  - ë©”ì‹œì§€ê°€ ì €ì¥ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤!")
        
        return {
            'statusCode': 200,
            'body': json.dumps({'message': 'ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ'})
        }
        
    except Exception as e:
        print(f"ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì˜¤ë¥˜: {traceback.format_exc()}")
        send_error(connection_id, f"ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {str(e)}")
        return {
            'statusCode': 500,
            'body': json.dumps({'error': str(e)})
        }

def handle_stepwise_execution(connection_id, user_input, prompt_cards, chat_history, conversation_id, user_sub):
    """
    ë‹¨ê³„ë³„ í”„ë¡¬í”„íŠ¸ ì‹¤í–‰ ë° ì‚¬ê³ ê³¼ì • ìŠ¤íŠ¸ë¦¬ë°
    """
    try:
        # ì‹œì‘ ë©”ì‹œì§€
        send_message(connection_id, {
            "type": "start",
            "message": "ë‹¨ê³„ë³„ í”„ë¡¬í”„íŠ¸ ì‹¤í–‰ì„ ì‹œì‘í•©ë‹ˆë‹¤."
        })
        
        full_response = ""
        current_context = {
            'chat_history': chat_history
        }
        
        # ê° í”„ë¡¬í”„íŠ¸ ì¹´ë“œë¥¼ ë‹¨ê³„ë³„ë¡œ ì‹¤í–‰
        for idx, card in enumerate(prompt_cards):
            step_name = card.get('title', f'Step {idx + 1}')
            threshold = float(card.get('threshold', 0.7))
            
            # ì‚¬ê³ ê³¼ì • ì‹œì‘
            send_message(connection_id, {
                "type": "thought_process",
                "step": step_name,
                "thought": f"{step_name} ë‹¨ê³„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.",
                "reasoning": "í”„ë¡¬í”„íŠ¸ ì¹´ë“œì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.",
                "confidence": 1.0,
                "decision": "PROCEED"
            })
            
            # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            step_prompt = build_step_prompt(card, user_input, current_context)
            
            # Bedrock í˜¸ì¶œ
            request_body = {
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 2048,
                "messages": [{"role": "user", "content": step_prompt}],
                "temperature": 0.1,
                "top_p": 0.9,
            }
            
            response = bedrock_client.invoke_model(
                modelId=MODEL_ID,
                body=json.dumps(request_body)
            )
            
            response_body = json.loads(response['body'].read())
            step_response = response_body.get('content', [{}])[0].get('text', '')
            
            # ì‘ë‹µ ë¶„ì„ ë° ì‹ ë¢°ë„ ê³„ì‚°
            confidence = analyze_response_confidence(step_response, card)
            
            # ë‹¨ê³„ ê²°ê³¼ ìŠ¤íŠ¸ë¦¬ë°
            send_message(connection_id, {
                "type": "step_result",
                "step": step_name,
                "response": step_response,
                "confidence": confidence,
                "threshold": threshold
            })
            
            # ì„ê³„ê°’ í‰ê°€
            if confidence < threshold:
                # ì‚¬ê³ ê³¼ì •: ì„ê³„ê°’ ë¯¸ë‹¬
                send_message(connection_id, {
                    "type": "thought_process",
                    "step": step_name,
                    "thought": f"ì‹ ë¢°ë„({confidence:.2f})ê°€ ì„ê³„ê°’({threshold:.2f})ë³´ë‹¤ ë‚®ìŠµë‹ˆë‹¤.",
                    "reasoning": "ì‘ë‹µì˜ í’ˆì§ˆì´ ê¸°ì¤€ì— ë¯¸ë‹¬í•˜ì—¬ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
                    "confidence": confidence,
                    "decision": "STOP"
                })
                break
            else:
                # ì‚¬ê³ ê³¼ì •: ë‹¤ìŒ ë‹¨ê³„ ì§„í–‰
                send_message(connection_id, {
                    "type": "thought_process", 
                    "step": step_name,
                    "thought": f"ì‹ ë¢°ë„({confidence:.2f})ê°€ ì„ê³„ê°’({threshold:.2f})ì„ ì¶©ì¡±í•©ë‹ˆë‹¤.",
                    "reasoning": "ì‘ë‹µì´ ì¶©ë¶„íˆ ì‹ ë¢°í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.",
                    "confidence": confidence,
                    "decision": "CONTINUE"
                })
            
            # ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
            current_context[f'step_{idx}_result'] = step_response
            full_response = step_response  # ë§ˆì§€ë§‰ ì‘ë‹µì„ ìµœì¢… ì‘ë‹µìœ¼ë¡œ
        
        # ì™„ë£Œ ë©”ì‹œì§€
        send_message(connection_id, {
            "type": "complete",
            "response": full_response
        })
        
        # ëŒ€í™” ì €ì¥
        if conversation_id and user_sub:
            save_conversation_messages(conversation_id, user_sub, user_input, full_response)
        
        return {
            'statusCode': 200,
            'body': json.dumps({'message': 'ë‹¨ê³„ë³„ ì‹¤í–‰ ì™„ë£Œ'})
        }
        
    except Exception as e:
        print(f"ë‹¨ê³„ë³„ ì‹¤í–‰ ì˜¤ë¥˜: {traceback.format_exc()}")
        send_error(connection_id, f"ë‹¨ê³„ë³„ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")
        return {
            'statusCode': 500,
            'body': json.dumps({'error': str(e)})
        }

def build_step_prompt(card, user_input, context):
    """ë‹¨ê³„ë³„ í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
    base_prompt = card.get('content', '')
    
    # ì´ì „ ë‹¨ê³„ ê²°ê³¼ ì¶”ê°€
    context_parts = []
    for key, value in context.items():
        if key.startswith('step_') and key.endswith('_result'):
            step_num = key.split('_')[1]
            context_parts.append(f"[ì´ì „ ë‹¨ê³„ {int(step_num)+1} ê²°ê³¼]\n{value}")
    
    if context_parts:
        base_prompt += "\n\n" + "\n\n".join(context_parts)
    
    # ì‚¬ìš©ì ì…ë ¥ ì¶”ê°€
    base_prompt += f"\n\nì‚¬ìš©ì ìš”ì²­: {user_input}"
    
    return base_prompt

def analyze_response_confidence(response, card):
    """ì‘ë‹µ ì‹ ë¢°ë„ ë¶„ì„"""
    # ê¸°ë³¸ ì‹ ë¢°ë„
    confidence = 0.8
    
    # ì‘ë‹µ ê¸¸ì´ ê¸°ë°˜ ì¡°ì •
    if len(response) < 50:
        confidence -= 0.2
    elif len(response) > 500:
        confidence += 0.1
    
    # ê¸ì •/ë¶€ì • í‚¤ì›Œë“œ ì²´í¬
    positive_keywords = card.get('positive_keywords', ['ì™„ë£Œ', 'ì„±ê³µ', 'í™•ì¸'])
    negative_keywords = card.get('negative_keywords', ['ì‹¤íŒ¨', 'ì˜¤ë¥˜', 'ë¶ˆê°€ëŠ¥'])
    
    for keyword in positive_keywords:
        if keyword in response:
            confidence += 0.05
    
    for keyword in negative_keywords:
        if keyword in response:
            confidence -= 0.1
    
    # ë²”ìœ„ ì œí•œ
    return max(0.0, min(1.0, confidence))

def summarize_large_text(text, max_length=50000):
    """
    ëŒ€ìš©ëŸ‰ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•˜ì—¬ ì²˜ë¦¬ ê°€ëŠ¥í•œ í¬ê¸°ë¡œ ì¤„ì„
    """
    if len(text) <= max_length:
        return text
    
    print(f"ğŸ” [DEBUG] ëŒ€ìš©ëŸ‰ í…ìŠ¤íŠ¸ ìš”ì•½ ì‹œì‘: {len(text)}ì -> {max_length}ì")
    
    # í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ì£¼ìš” ë¶€ë¶„ë§Œ ì¶”ì¶œ
    chunk_size = max_length // 3
    
    # ì‹œì‘, ì¤‘ê°„, ë ë¶€ë¶„ ì¶”ì¶œ
    start_chunk = text[:chunk_size]
    middle_start = len(text) // 2 - chunk_size // 2
    middle_chunk = text[middle_start:middle_start + chunk_size]
    end_chunk = text[-chunk_size:]
    
    summarized = f"{start_chunk}\n\n[... ì¤‘ê°„ ë‚´ìš© ìƒëµ ...]\n\n{middle_chunk}\n\n[... ì¤‘ê°„ ë‚´ìš© ìƒëµ ...]\n\n{end_chunk}"
    
    print(f"âœ… [DEBUG] í…ìŠ¤íŠ¸ ìš”ì•½ ì™„ë£Œ: {len(summarized)}ì")
    return summarized

def build_final_prompt(user_input, chat_history, prompt_cards):
    """
    í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì „ì†¡ëœ í”„ë¡¬í”„íŠ¸ ì¹´ë“œì™€ ì±„íŒ… íˆìŠ¤í† ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì¢… í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    """
    try:
        print(f"WebSocket í”„ë¡¬í”„íŠ¸ êµ¬ì„± ì‹œì‘")
        print(f"ì „ë‹¬ë°›ì€ í”„ë¡¬í”„íŠ¸ ì¹´ë“œ ìˆ˜: {len(prompt_cards)}")
        print(f"ì „ë‹¬ë°›ì€ ì±„íŒ… íˆìŠ¤í† ë¦¬ ìˆ˜: {len(chat_history)}")
        print(f"ì‚¬ìš©ì ì…ë ¥ ê¸¸ì´: {len(user_input)}ì")
        
        # ëŒ€ìš©ëŸ‰ í…ìŠ¤íŠ¸ ì²˜ë¦¬
        MAX_INPUT_LENGTH = 150000  # 150KBë¡œ ì¦ê°€ (generate.pyì™€ ë™ì¼)
        if len(user_input) > MAX_INPUT_LENGTH:
            print(f"âš ï¸ [WARNING] ì‚¬ìš©ì ì…ë ¥ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤. ìš”ì•½í•©ë‹ˆë‹¤.")
            user_input = summarize_large_text(user_input, MAX_INPUT_LENGTH)
        
        # í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì „ì†¡ëœ í”„ë¡¬í”„íŠ¸ ì¹´ë“œ ì‚¬ìš©
        system_prompt_parts = []
        for card in prompt_cards:
            prompt_text = card.get('prompt_text', '').strip()
            if prompt_text:
                title = card.get('title', 'Untitled')
                print(f"WebSocket í”„ë¡¬í”„íŠ¸ ì¹´ë“œ ì ìš©: '{title}' ({len(prompt_text)}ì)")
                system_prompt_parts.append(prompt_text)
        
        system_prompt = "\n\n".join(system_prompt_parts)
        print(f"WebSocket ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(system_prompt)}ì")
        
        # ì±„íŒ… íˆìŠ¤í† ë¦¬ êµ¬ì„± (ìµœê·¼ 10ê°œë§Œ)
        recent_history = chat_history[-10:] if len(chat_history) > 10 else chat_history
        history_parts = []
        for msg in recent_history:
            role = msg.get('role', '')
            content = msg.get('content', '')
            if role and content:
                # íˆìŠ¤í† ë¦¬ ë©”ì‹œì§€ë„ ê¸¸ì´ ì œí•œ
                if len(content) > 1000:
                    content = content[:1000] + "..."
                if role == 'user':
                    history_parts.append(f"Human: {content}")
                elif role == 'assistant':
                    history_parts.append(f"Assistant: {content}")
        
        history_str = "\n\n".join(history_parts)
        print(f"WebSocket ì±„íŒ… íˆìŠ¤í† ë¦¬ ê¸¸ì´: {len(history_str)}ì")
        
        # ìµœì¢… í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt_parts = []
        
        # 1. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì—­í• , ì§€ì¹¨ ë“±)
        if system_prompt:
            prompt_parts.append(system_prompt)
        
        # 2. ëŒ€í™” íˆìŠ¤í† ë¦¬
        if history_str:
            prompt_parts.append(history_str)
        
        # 3. í˜„ì¬ ì‚¬ìš©ì ì…ë ¥
        prompt_parts.append(f"Human: {user_input}")
        prompt_parts.append("Assistant:")
        
        final_prompt = "\n\n".join(prompt_parts)
        print(f"WebSocket ìµœì¢… í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(final_prompt)}ì")
        
        # ìµœì¢… í”„ë¡¬í”„íŠ¸ë„ í¬ê¸° ì œí•œ
        MAX_PROMPT_LENGTH = 180000  # 180KB (Claude í† í° ì œí•œ ê³ ë ¤)
        if len(final_prompt) > MAX_PROMPT_LENGTH:
            print(f"âš ï¸ [WARNING] ìµœì¢… í”„ë¡¬í”„íŠ¸ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤. ì˜ë¼ì„œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
            # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì™€ ì‚¬ìš©ì ì…ë ¥ë§Œ ì‚¬ìš©
            final_prompt = f"{system_prompt}\n\nHuman: {user_input}\n\nAssistant:"
            if len(final_prompt) > MAX_PROMPT_LENGTH:
                # ê·¸ë˜ë„ í¬ë©´ ì‚¬ìš©ì ì…ë ¥ë§Œ
                final_prompt = f"Human: {user_input[:MAX_PROMPT_LENGTH-20]}\n\nAssistant:"
        
        return final_prompt
        
    except Exception as e:
        print(f"WebSocket í”„ë¡¬í”„íŠ¸ êµ¬ì„± ì˜¤ë¥˜: {traceback.format_exc()}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ë°˜í™˜
        return f"Human: {user_input[:50000]}\n\nAssistant:"

def send_message(connection_id, message):
    """
    WebSocket í´ë¼ì´ì–¸íŠ¸ë¡œ ë©”ì‹œì§€ ì „ì†¡
    """
    try:
        apigateway_client.post_to_connection(
            ConnectionId=connection_id,
            Data=json.dumps(message)
        )
    except Exception as e:
        print(f"ë©”ì‹œì§€ ì „ì†¡ ì‹¤íŒ¨: {connection_id}, ì˜¤ë¥˜: {str(e)}")
        # ì—°ê²°ì´ ëŠì–´ì§„ ê²½ìš° DynamoDBì—ì„œ ì œê±°
        if 'GoneException' in str(e):
            try:
                dynamodb_client.delete_item(
                    TableName=CONNECTIONS_TABLE,
                    Key={'connectionId': {'S': connection_id}}
                )
            except:
                pass

def send_error(connection_id, error_message):
    """
    ì˜¤ë¥˜ ë©”ì‹œì§€ ì „ì†¡
    """
    send_message(connection_id, {
        "type": "error",
        "message": error_message,
        "timestamp": datetime.utcnow().isoformat()
    })
    
    return {
        'statusCode': 400,
        'body': json.dumps({'error': error_message})
    }

def save_conversation_messages(conversation_id, user_sub, user_input, assistant_response):
    """
    ëŒ€í™” ë©”ì‹œì§€ë¥¼ DynamoDBì— ì €ì¥
    """
    try:
        print(f"ğŸ” [DEBUG] save_conversation_messages ì‹œì‘:")
        print(f"  - conversation_id: {conversation_id}")
        print(f"  - user_sub: {user_sub}")
        
        now = datetime.now(timezone.utc)
        user_timestamp = now.isoformat()
        assistant_timestamp = (now.replace(microsecond=now.microsecond + 1000)).isoformat()
        
        # Calculate TTL (180 days from now)
        ttl = int(now.timestamp() + (180 * 24 * 60 * 60))
        
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥
        user_message = {
            'PK': f'CONV#{conversation_id}',
            'SK': f'TS#{user_timestamp}',
            'role': 'user',
            'content': user_input,
            'tokenCount': estimate_token_count(user_input),
            'ttl': ttl
        }
        
        # ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ì €ì¥
        assistant_message = {
            'PK': f'CONV#{conversation_id}',
            'SK': f'TS#{assistant_timestamp}',
            'role': 'assistant', 
            'content': assistant_response,
            'tokenCount': estimate_token_count(assistant_response),
            'ttl': ttl
        }
        
        print(f"ğŸ” [DEBUG] DynamoDBì— ì €ì¥í•  ë©”ì‹œì§€ë“¤:")
        print(f"  - User message PK: {user_message['PK']}")
        print(f"  - User message SK: {user_message['SK']}")
        print(f"  - Assistant message PK: {assistant_message['PK']}")
        print(f"  - Assistant message SK: {assistant_message['SK']}")
        
        # ë°°ì¹˜ë¡œ ë©”ì‹œì§€ ì €ì¥
        with messages_table.batch_writer() as batch:
            batch.put_item(Item=user_message)
            batch.put_item(Item=assistant_message)
        
        # ëŒ€í™” í™œë™ ì‹œê°„ ë° í† í° ì¹´ìš´íŠ¸ ì—…ë°ì´íŠ¸
        total_tokens = user_message['tokenCount'] + assistant_message['tokenCount']
        update_conversation_activity(conversation_id, user_sub, total_tokens)
        
        print(f"ğŸ” [DEBUG] ë©”ì‹œì§€ ì €ì¥ ì™„ë£Œ: {conversation_id}, í† í°: {total_tokens}")
        
    except Exception as e:
        print(f"ë©”ì‹œì§€ ì €ì¥ ì˜¤ë¥˜: {str(e)}")
        print(traceback.format_exc())

def update_conversation_activity(conversation_id, user_sub, token_count):
    """
    ëŒ€í™”ì˜ ë§ˆì§€ë§‰ í™œë™ ì‹œê°„ê³¼ í† í° í•©ê³„ ì—…ë°ì´íŠ¸
    """
    try:
        now = datetime.now(timezone.utc).isoformat()
        
        conversations_table.update_item(
            Key={
                'PK': f'USER#{user_sub}',
                'SK': f'CONV#{conversation_id}'
            },
            UpdateExpression='SET lastActivityAt = :activity, tokenSum = tokenSum + :tokens',
            ExpressionAttributeValues={
                ':activity': now,
                
                ':tokens': token_count
            }
        )
        
    except Exception as e:
        print(f"ëŒ€í™” í™œë™ ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {str(e)}")

def estimate_token_count(text):
    """
    ê°„ë‹¨í•œ í† í° ìˆ˜ ì¶”ì • (ëŒ€ëµ 4ì = 1í† í°)
    ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” tokenizer ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ê¶Œì¥
    """
    if not text:
        return 0
    return max(1, len(text) // 4)

def _start_parallel_chunk_processing(connection_id, chunk_id, total_chunks):
    """
    ë³‘ë ¬ ì²­í¬ ì²˜ë¦¬ ì‹œì‘
    """
    try:
        # Lambda ë¹„ë™ê¸° í˜¸ì¶œì„ í†µí•œ ë³‘ë ¬ ì²˜ë¦¬
        lambda_client = boto3.client('lambda')
        function_name = os.environ.get('PARALLEL_PROCESSOR_FUNCTION', 'title-generator-parallel-processor')
        
        # ë³‘ë ¬ ì²˜ë¦¬ ì‘ì—… ìƒì„±
        payload = {
            'action': 'process_parallel_chunks',
            'chunkId': chunk_id,
            'totalChunks': total_chunks,
            'connectionId': connection_id
        }
        
        # ë¹„ë™ê¸° í˜¸ì¶œ
        lambda_client.invoke(
            FunctionName=function_name,
            InvocationType='Event',  # ë¹„ë™ê¸°
            Payload=json.dumps(payload)
        )
        
        print(f"ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘: chunk_id={chunk_id}, total_chunks={total_chunks}")
        
    except Exception as e:
        print(f"ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘ ì˜¤ë¥˜: {e}")
        # í´ë°±: ê¸°ì¡´ ìˆœì°¨ ì²˜ë¦¬ ë°©ì‹ ìœ ì§€