"""
LangGraph ê¸°ë°˜ ì‚¬ê³ ê³¼ì • ì¶”ì  ì›Œí¬í”Œë¡œìš°
"""
from typing import TypedDict, List, Dict, Any
from langgraph.graph import StateGraph, END
from langchain_aws import ChatBedrock
import json

class ThinkingState(TypedDict):
    """ì‚¬ê³ ê³¼ì • ìƒíƒœ ê´€ë¦¬"""
    user_input: str
    prompt_cards: List[Dict[str, Any]]
    current_context: Dict[str, Any]
    thinking_steps: List[Dict[str, Any]]
    final_answer: str
    confidence_score: float
    websocket_connection: str

def analyze_question(state: ThinkingState) -> ThinkingState:
    """1ë‹¨ê³„: ì‚¬ìš©ì ì§ˆë¬¸ ë¶„ì„"""
    user_input = state["user_input"]
    connection_id = state["websocket_connection"]
    
    # WebSocketìœ¼ë¡œ ì§„í–‰ìƒí™© ì „ì†¡
    send_thinking_step(connection_id, {
        "type": "thinking_step",
        "step": "question_analysis",
        "title": "ğŸ” ì§ˆë¬¸ ë¶„ì„ ì¤‘...",
        "thought": f"ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤: '{user_input[:50]}...'",
        "reasoning": "ì§ˆë¬¸ì˜ ì˜ë„ì™€ í•µì‹¬ í‚¤ì›Œë“œë¥¼ íŒŒì•…í•˜ê³  ìˆìŠµë‹ˆë‹¤.",
        "status": "processing"
    })
    
    # ì§ˆë¬¸ ë¶„ì„ ë¡œì§
    analysis = {
        "question_type": classify_question_type(user_input),
        "key_entities": extract_key_entities(user_input),
        "intent": analyze_user_intent(user_input),
        "complexity": assess_complexity(user_input)
    }
    
    state["thinking_steps"].append({
        "step": "question_analysis",
        "result": analysis,
        "confidence": 0.8
    })
    
    send_thinking_step(connection_id, {
        "type": "thinking_step",
        "step": "question_analysis",
        "title": "âœ… ì§ˆë¬¸ ë¶„ì„ ì™„ë£Œ",
        "thought": f"ì§ˆë¬¸ ìœ í˜•: {analysis['question_type']}, ë³µì¡ë„: {analysis['complexity']}",
        "reasoning": "ì§ˆë¬¸ì˜ êµ¬ì¡°ì™€ ìš”êµ¬ì‚¬í•­ì„ íŒŒì•…í–ˆìŠµë‹ˆë‹¤.",
        "status": "completed"
    })
    
    return state

def select_relevant_prompts(state: ThinkingState) -> ThinkingState:
    """2ë‹¨ê³„: ê´€ë ¨ í”„ë¡¬í”„íŠ¸ ì¹´ë“œ ì„ íƒ"""
    connection_id = state["websocket_connection"]
    prompt_cards = state["prompt_cards"]
    question_analysis = state["thinking_steps"][-1]["result"]
    
    send_thinking_step(connection_id, {
        "type": "thinking_step", 
        "step": "prompt_selection",
        "title": "ğŸ¯ ì ì ˆí•œ í”„ë¡¬í”„íŠ¸ ì„ íƒ ì¤‘...",
        "thought": f"{len(prompt_cards)}ê°œì˜ í”„ë¡¬í”„íŠ¸ ì¹´ë“œì—ì„œ ìµœì ì˜ ì¡°í•©ì„ ì°¾ê³  ìˆìŠµë‹ˆë‹¤.",
        "reasoning": "ì§ˆë¬¸ ìœ í˜•ì— ë§ëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ì„ ë³„í•©ë‹ˆë‹¤.",
        "status": "processing"
    })
    
    # í”„ë¡¬í”„íŠ¸ ì„ íƒ ë¡œì§
    selected_prompts = []
    for card in prompt_cards:
        relevance_score = calculate_relevance(card, question_analysis)
        if relevance_score > 0.5:
            selected_prompts.append({
                "card": card,
                "relevance": relevance_score
            })
    
    # ê´€ë ¨ë„ ìˆœìœ¼ë¡œ ì •ë ¬
    selected_prompts.sort(key=lambda x: x["relevance"], reverse=True)
    
    state["thinking_steps"].append({
        "step": "prompt_selection",
        "result": selected_prompts,
        "confidence": 0.9
    })
    
    send_thinking_step(connection_id, {
        "type": "thinking_step",
        "step": "prompt_selection", 
        "title": "âœ… í”„ë¡¬í”„íŠ¸ ì„ íƒ ì™„ë£Œ",
        "thought": f"{len(selected_prompts)}ê°œì˜ ê´€ë ¨ í”„ë¡¬í”„íŠ¸ë¥¼ ì„ íƒí–ˆìŠµë‹ˆë‹¤.",
        "reasoning": "ê°€ì¥ ì í•©í•œ í”„ë¡¬í”„íŠ¸ ì¡°í•©ì„ ê²°ì •í–ˆìŠµë‹ˆë‹¤.",
        "status": "completed"
    })
    
    return state

def generate_initial_response(state: ThinkingState) -> ThinkingState:
    """3ë‹¨ê³„: ì´ˆê¸° ë‹µë³€ ìƒì„±"""
    connection_id = state["websocket_connection"]
    
    send_thinking_step(connection_id, {
        "type": "thinking_step",
        "step": "initial_generation", 
        "title": "ğŸ’­ ì´ˆê¸° ë‹µë³€ ìƒì„± ì¤‘...",
        "thought": "ì„ íƒëœ í”„ë¡¬í”„íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì²« ë²ˆì§¸ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.",
        "reasoning": "ì²´ê³„ì ì¸ ì‚¬ê³  ê³¼ì •ì„ í†µí•´ ë‹µë³€ì„ êµ¬ì„±í•©ë‹ˆë‹¤.",
        "status": "processing"
    })
    
    # Claude API í˜¸ì¶œë¡œ ì´ˆê¸° ë‹µë³€ ìƒì„±
    selected_prompts = state["thinking_steps"][-1]["result"]
    system_prompt = build_system_prompt(selected_prompts)
    
    llm = ChatBedrock(
        model_id="apac.anthropic.claude-sonnet-4-20250514-v1:0",
        model_kwargs={"temperature": 0.3, "max_tokens": 65536}
    )
    
    initial_response = llm.invoke([
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": state["user_input"]}
    ])
    
    state["thinking_steps"].append({
        "step": "initial_generation",
        "result": initial_response.content,
        "confidence": 0.7
    })
    
    send_thinking_step(connection_id, {
        "type": "thinking_step",
        "step": "initial_generation",
        "title": "âœ… ì´ˆê¸° ë‹µë³€ ìƒì„± ì™„ë£Œ", 
        "thought": "ì²« ë²ˆì§¸ ë‹µë³€ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.",
        "reasoning": "ì´ì œ ì´ ë‹µë³€ì„ ê²€í† í•˜ê³  ê°œì„ í•  ì°¨ë¡€ì…ë‹ˆë‹¤.",
        "status": "completed"
    })
    
    return state

def self_critique(state: ThinkingState) -> ThinkingState:
    """4ë‹¨ê³„: ìê°€ ë¹„íŒ ë° ê²€í† """
    connection_id = state["websocket_connection"]
    initial_response = state["thinking_steps"][-1]["result"]
    
    send_thinking_step(connection_id, {
        "type": "thinking_step",
        "step": "self_critique",
        "title": "ğŸ¤” ë‹µë³€ ìê°€ ê²€í†  ì¤‘...", 
        "thought": "ìƒì„±ëœ ë‹µë³€ì˜ í’ˆì§ˆê³¼ ì •í™•ì„±ì„ ê²€í† í•©ë‹ˆë‹¤.",
        "reasoning": "ë” ë‚˜ì€ ë‹µë³€ì„ ìœ„í•´ ë¹„íŒì  ì‚¬ê³ ë¥¼ ì ìš©í•©ë‹ˆë‹¤.",
        "status": "processing"
    })
    
    # ìê°€ ë¹„íŒ í”„ë¡¬í”„íŠ¸
    critique_prompt = f"""
    ë‹¤ìŒ ë‹µë³€ì„ ë¹„íŒì ìœ¼ë¡œ ê²€í† í•˜ì„¸ìš”:
    
    ì§ˆë¬¸: {state["user_input"]}
    ë‹µë³€: {initial_response}
    
    ê²€í†  ê¸°ì¤€:
    1. ì •í™•ì„±: ì‚¬ì‹¤ì  ì˜¤ë¥˜ê°€ ìˆëŠ”ê°€?
    2. ì™„ì„±ë„: ì§ˆë¬¸ì— ì¶©ë¶„íˆ ë‹µí–ˆëŠ”ê°€?
    3. ëª…í™•ì„±: ì´í•´í•˜ê¸° ì‰¬ìš´ê°€?
    4. ê´€ë ¨ì„±: ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆëŠ”ê°€?
    
    ê°œì„ ì ê³¼ ì ìˆ˜(1-10)ë¥¼ ì œì‹œí•˜ì„¸ìš”.
    """
    
    llm = ChatBedrock(
        model_id="apac.anthropic.claude-sonnet-4-20250514-v1:0",
        model_kwargs={"temperature": 0.1, "max_tokens": 65536}
    )
    
    critique_result = llm.invoke([
        {"role": "user", "content": critique_prompt}
    ])
    
    state["thinking_steps"].append({
        "step": "self_critique", 
        "result": critique_result.content,
        "confidence": 0.8
    })
    
    send_thinking_step(connection_id, {
        "type": "thinking_step",
        "step": "self_critique",
        "title": "âœ… ìê°€ ê²€í†  ì™„ë£Œ",
        "thought": "ë‹µë³€ì˜ ê°•ì ê³¼ ê°œì„ ì ì„ íŒŒì•…í–ˆìŠµë‹ˆë‹¤.",
        "reasoning": "ê²€í†  ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ ê°œì„  ì—¬ë¶€ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.",
        "status": "completed"
    })
    
    return state

def should_refine(state: ThinkingState) -> str:
    """ê°œì„  í•„ìš”ì„± íŒë‹¨"""
    critique = state["thinking_steps"][-1]["result"]
    
    # ì ìˆ˜ ì¶”ì¶œ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
    if "ì ìˆ˜" in critique and any(str(i) in critique for i in range(1, 7)):
        return "refine"
    elif "ìš°ìˆ˜" in critique or "ì™„ë²½" in critique:
        return "finalize"
    else:
        return "refine"

def refine_answer(state: ThinkingState) -> ThinkingState:
    """5ë‹¨ê³„: ë‹µë³€ ê°œì„ """
    connection_id = state["websocket_connection"]
    initial_response = state["thinking_steps"][2]["result"]
    critique = state["thinking_steps"][-1]["result"]
    
    send_thinking_step(connection_id, {
        "type": "thinking_step",
        "step": "refinement",
        "title": "ğŸ”§ ë‹µë³€ ê°œì„  ì¤‘...",
        "thought": "ë¹„íŒì  ê²€í† ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ê°œì„ í•©ë‹ˆë‹¤.",
        "reasoning": "ë” ì •í™•í•˜ê³  ì™„ì„±ë„ ë†’ì€ ë‹µë³€ì„ ë§Œë“¤ì–´ê°‘ë‹ˆë‹¤.",
        "status": "processing"
    })
    
    refinement_prompt = f"""
    ë‹¤ìŒ ë‹µë³€ì„ ê°œì„ í•˜ì„¸ìš”:
    
    ì›ë˜ ì§ˆë¬¸: {state["user_input"]}
    ì´ˆê¸° ë‹µë³€: {initial_response}
    ê²€í†  ì˜ê²¬: {critique}
    
    ê²€í†  ì˜ê²¬ì„ ë°˜ì˜í•˜ì—¬ ë” ë‚˜ì€ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.
    """
    
    llm = ChatBedrock(
        model_id="apac.anthropic.claude-sonnet-4-20250514-v1:0", 
        model_kwargs={"temperature": 0.2, "max_tokens": 65536}
    )
    
    refined_response = llm.invoke([
        {"role": "user", "content": refinement_prompt}
    ])
    
    state["final_answer"] = refined_response.content
    state["confidence_score"] = 0.9
    
    state["thinking_steps"].append({
        "step": "refinement",
        "result": refined_response.content,
        "confidence": 0.9
    })
    
    send_thinking_step(connection_id, {
        "type": "thinking_step",
        "step": "refinement", 
        "title": "âœ… ë‹µë³€ ê°œì„  ì™„ë£Œ",
        "thought": "ìµœì¢… ê°œì„ ëœ ë‹µë³€ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.",
        "reasoning": "ì‚¬ê³ ê³¼ì •ì„ í†µí•´ ê³ í’ˆì§ˆ ë‹µë³€ì„ ì™„ì„±í–ˆìŠµë‹ˆë‹¤.",
        "status": "completed"
    })
    
    return state

def finalize_answer(state: ThinkingState) -> ThinkingState:
    """ìµœì¢…ë‹µë³€ í™•ì •"""
    if not state.get("final_answer"):
        # ê°œì„  ë‹¨ê³„ë¥¼ ê±°ì¹˜ì§€ ì•Šì€ ê²½ìš°
        state["final_answer"] = state["thinking_steps"][2]["result"]
        state["confidence_score"] = 0.8
    
    return state

# ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ ìƒì„±
def create_thinking_workflow():
    """ì‚¬ê³ ê³¼ì • ì¶”ì  ì›Œí¬í”Œë¡œìš° ìƒì„±"""
    
    workflow = StateGraph(ThinkingState)
    
    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("analyze_question", analyze_question)
    workflow.add_node("select_prompts", select_relevant_prompts) 
    workflow.add_node("generate_initial", generate_initial_response)
    workflow.add_node("self_critique", self_critique)
    workflow.add_node("refine_answer", refine_answer)
    workflow.add_node("finalize", finalize_answer)
    
    # ì—£ì§€ ì—°ê²°
    workflow.set_entry_point("analyze_question")
    workflow.add_edge("analyze_question", "select_prompts")
    workflow.add_edge("select_prompts", "generate_initial") 
    workflow.add_edge("generate_initial", "self_critique")
    
    # ì¡°ê±´ë¶€ ì—£ì§€
    workflow.add_conditional_edges(
        "self_critique",
        should_refine,
        {
            "refine": "refine_answer",
            "finalize": "finalize"
        }
    )
    
    workflow.add_edge("refine_answer", "finalize")
    workflow.add_edge("finalize", END)
    
    return workflow.compile()

# í—¬í¼ í•¨ìˆ˜ë“¤
def classify_question_type(question: str) -> str:
    """ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜"""
    if "?" in question:
        if any(word in question.lower() for word in ["what", "ë¬´ì—‡", "ë­"]):
            return "ì •ì˜/ì„¤ëª…"
        elif any(word in question.lower() for word in ["how", "ì–´ë–»ê²Œ"]):
            return "ë°©ë²•/ì ˆì°¨"
        elif any(word in question.lower() for word in ["why", "ì™œ"]):
            return "ì´ìœ /ì›ì¸"
    return "ì¼ë°˜"

def extract_key_entities(question: str) -> List[str]:
    """í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ NLP ì‚¬ìš©)
    words = question.split()
    return [word for word in words if len(word) > 2]

def analyze_user_intent(question: str) -> str:
    """ì‚¬ìš©ì ì˜ë„ ë¶„ì„"""
    if any(word in question.lower() for word in ["help", "ë„ì›€", "ë„ì™€"]):
        return "ë„ì›€ìš”ì²­"
    elif any(word in question.lower() for word in ["explain", "ì„¤ëª…"]):
        return "ì„¤ëª…ìš”ì²­"
    return "ì •ë³´ìš”ì²­"

def assess_complexity(question: str) -> str:
    """ì§ˆë¬¸ ë³µì¡ë„ í‰ê°€"""
    word_count = len(question.split())
    if word_count < 5:
        return "ë‹¨ìˆœ"
    elif word_count < 15:
        return "ë³´í†µ"
    else:
        return "ë³µì¡"

def calculate_relevance(card: Dict, analysis: Dict) -> float:
    """í”„ë¡¬í”„íŠ¸ ì¹´ë“œ ê´€ë ¨ë„ ê³„ì‚°"""
    # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­ ê¸°ë°˜ ê´€ë ¨ë„ ê³„ì‚°
    card_text = card.get("prompt_text", "").lower()
    question_entities = analysis.get("key_entities", [])
    
    matches = sum(1 for entity in question_entities if entity.lower() in card_text)
    return matches / max(len(question_entities), 1)

def build_system_prompt(selected_prompts: List[Dict]) -> str:
    """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
    prompt_parts = []
    for prompt_info in selected_prompts:
        card = prompt_info["card"]
        prompt_parts.append(card.get("prompt_text", ""))
    
    return "\n\n".join(prompt_parts)

def send_thinking_step(connection_id: str, step_data: Dict):
    """WebSocketìœ¼ë¡œ ì‚¬ê³ ê³¼ì • ì „ì†¡"""
    # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” apigateway_client ì‚¬ìš©
    print(f"WebSocket [{connection_id}]: {step_data}")